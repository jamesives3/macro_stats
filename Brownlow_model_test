install.packages("devtools")

devtools::install_github("jimmyday12/fitzroy")


#brownlow predictor 
stats_afltables <- fetch_player_stats_afltables(2014:2023)

afl_stats <- fetch_player_stats_fryzigg(season = 2014:2023)
view(afl_stats)

View(stats_afltables) 
install.packages("forcats")
library(forcats)

#get brownlow votes leaderboard for the last 10 years 
stats_afltables %>%
  mutate(year = year(Date),
         player_name = paste0(First.name, " ", Surname)) %>% 
  group_by(year, player_name, Playing.for) %>% 
  summarise(brownlow_total = sum(Brownlow.Votes)) %>% 
  ungroup() %>% 
  group_by(year) %>% 
  slice_max(n = 5, order_by = brownlow_total) %>% 
  mutate(
    year = as.factor(year),
  ) %>% 
  ggplot(aes(brownlow_total, player_name, fill = year)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~ year, scales = "free", ncol = 3) +
  geom_text(aes(label = brownlow_total), size = 3, hjust = 1.1)

view(stats_afltables)


#stats (disposals, inside 50s, contested possessions, margin, clearances, fantasy points, rating points
afl <- afl_stats %>% 
  mutate(year = year(match_date),
         player_name = paste0(player_first_name, " ", player_last_name),
         match_margin = ifelse(player_team == match_winner, match_margin, -match_margin),
         game_outcome = ifelse(player_team == match_winner, "win", "loss")) %>% 
  select(player_name, match_date, disposals, contested_possessions, 
         clearances, inside_fifties, game_outcome, 
         match_margin, rating_points, supercoach_score, brownlow_votes)
  
#train set
train <- afl %>% 
  filter(year(match_date) < 2023) %>% 
  mutate(brownlow_votes = as.factor(brownlow_votes))
  
#test 
test <- afl %>% 
  filter(year(match_date) == 2023)

#add additional libraries
install.packages("rsample")
install.packages("tidymodels")
install.packages("xgboost")
library(rsample)
library(tidymodels)
library(xgboost)


  set.seed(100)
  afl_splits <- initial_split(train)
  afl_train <- training(afl_splits)
  afl_test <- testing(afl_splits)
  afl_cv <- vfold_cv(afl_train, v = 5) #5 fold cross validation 
  
  #use xg boost model
  xg_spec <- 
    boost_tree(
      mode = "classification",
      engine = "xgboost",
      mtry = tune(), #tune means we will have a bunch of values for these parameters of the model and determine which combination of these parameters provide the greatest accuracy (how well is the model predicting hte data)
      trees = tune(),
      learn_rate = 0.02
    )
  
  xg_rec <- recipe(brownlow_votes ~ ., data = afl_train) %>% 
    step_rm(match_date, player_name) %>% 
    step_dummy(all_nominal_predictors()) %>% 
    step_impute_median(all_numeric_predictors())
  
  xg_wf <-
    workflow() %>% 
    add_model(xg_spec) %>% 
    add_recipe(xg_rec)
  
  xg_tune <- xg_wf %>% 
    tune_grid(resamples = afl_cv, 
              metric = metric_set(m_log_loss),
              control = control_grid(save_pred = TRUE),
              grid = crossing(mtry = 3:6,
                              trees = seq(300,800,50)))
xg_tune %>% 
  autoplot()
  
xg_tune %>% 
  collect_metrics() %>% 
  arrange(mean)

xg_last <- xg_wf %>% 
  finalize_workflow(select_best(xg_tune)) %>% 
  last_fit(afl_splits)

install.packages("vip")
library(vip)

xg_last %>% 
  extract_workflow() %>%
  extract_fit_parsnip() %>% 
  vip::vip(num_features = 20)


xg_fit_final <- xg_wf %>% 
  finalize_workflow(select_best(xg_tune)) %>% 
  fit(train)

xg_pred <- xg_fit_final %>% 
  augment(test)

xg_pred %>% 
  select(contains("_name"), contains(".pred")) %>% 
  mutate(exp_votes = 0*.pred_0 + 1*.pred_1 + 2*.pred_2 + 3*.pred_3) %>% 
  group_by(player_name) %>% 
  summarize(exp_total_votes = sum(exp_votes)) %>% 
  arrange(-exp_total_votes)